{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset\n",
    "ames_housing = pd.read_csv(\"ames_housing_no_missing.csv\")\n",
    "target_name = \"SalePrice\"\n",
    "data = ames_housing.drop(columns=target_name)\n",
    "target = ames_housing[target_name]\n",
    "\n",
    "numerical_features = [\n",
    "    \"LotFrontage\", \"LotArea\", \"MasVnrArea\", \"BsmtFinSF1\", \"BsmtFinSF2\",\n",
    "    \"BsmtUnfSF\", \"TotalBsmtSF\", \"1stFlrSF\", \"2ndFlrSF\", \"LowQualFinSF\",\n",
    "    \"GrLivArea\", \"BedroomAbvGr\", \"KitchenAbvGr\", \"TotRmsAbvGrd\", \"Fireplaces\",\n",
    "    \"GarageCars\", \"GarageArea\", \"WoodDeckSF\", \"OpenPorchSF\", \"EnclosedPorch\",\n",
    "    \"3SsnPorch\", \"ScreenPorch\", \"PoolArea\", \"MiscVal\",\n",
    "]\n",
    "\n",
    "data_numerical = data[numerical_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression is better than decision tree for 9 CV iterations out of 10 folds.\n"
     ]
    }
   ],
   "source": [
    "# Create models and compare them\n",
    "linear = make_pipeline(StandardScaler(), LinearRegression())\n",
    "tree = DecisionTreeRegressor()\n",
    "cv_linear = cross_validate(linear, data_numerical, target, cv=10)\n",
    "cv_tree = cross_validate(tree, data_numerical, target, cv=10)\n",
    "score_linear = cv_linear['test_score']\n",
    "score_tree = cv_tree['test_score']\n",
    "#print(score_linear > score_tree)\n",
    "print(\"Linear regression is better than decision tree for \"\n",
    "    f\"{sum(score_linear > score_tree)} CV iterations out of 10 folds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'max_depth': 7},\n",
       " {'max_depth': 7},\n",
       " {'max_depth': 6},\n",
       " {'max_depth': 7},\n",
       " {'max_depth': 6},\n",
       " {'max_depth': 8},\n",
       " {'max_depth': 6},\n",
       " {'max_depth': 7},\n",
       " {'max_depth': 8},\n",
       " {'max_depth': 8}]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inner and outer CV\n",
    "max_depth = [i for i in range(1,16)]\n",
    "param_grid = {'max_depth' : max_depth}\n",
    "search = GridSearchCV(tree, param_grid=param_grid, cv=10)\n",
    "cv_results_tree_optimal_depth = cross_validate(search, data_numerical, target, cv=10, return_estimator=True, n_jobs=2)\n",
    "best = [i.best_params_ for i in cv_results_tree_optimal_depth['estimator']]\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "A tree with an optimized depth is better than linear regression for 2 CV iterations out of 10 folds.\n"
     ]
    }
   ],
   "source": [
    "tree_opt = cv_results_tree_optimal_depth['test_score']\n",
    "for i,j in zip(tree_opt, scores_lr):\n",
    "    print(i>j)\n",
    "print(\n",
    "    \"A tree with an optimized depth is better than linear regression for \"\n",
    "    f\"{sum(cv_results_tree_optimal_depth['test_score'] > scores_lr)} CV \"\n",
    "    \"iterations out of 10 folds.\"\n",
    ")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6909853446444225"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy\n",
    "search = GridSearchCV(tree, params, cv=10)\n",
    "cv_results_tree_optimal_depth = cross_validate(\n",
    "    search, data_numerical, target, cv=10, return_estimator=True, n_jobs=-1,\n",
    ")\n",
    "cv_results_tree_optimal_depth[\"test_score\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler\n",
    "\n",
    "# Filtering categorical and numerical columns\n",
    "numerical_selector = selector(dtype_exclude=object)\n",
    "categorical_selector = selector(dtype_include=object)\n",
    "\n",
    "num_columns = numerical_selector(data)\n",
    "cat_columns = categorical_selector(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross validation accuracy: 0.767 +/- 0.067\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing\n",
    "cat_preprocessor = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
    "num_preprocessor = StandardScaler()\n",
    "\n",
    "preprocessor = ColumnTransformer([('one-hot-encoder', cat_preprocessor, cat_columns),\n",
    "                                 ('standar_scaler', num_preprocessor, num_columns)])\n",
    "\n",
    "# creating model\n",
    "from sklearn import set_config\n",
    "#set_config(display='diagram')\n",
    "model = make_pipeline(preprocessor, DecisionTreeRegressor(max_depth=7, random_state=0))\n",
    "\n",
    "# Crossval and accuracy\n",
    "cv = cross_validate(model, data, target, cv=10, return_estimator=True, n_jobs=2)\n",
    "scores = cv['test_score']\n",
    "print(f'Mean cross validation accuracy: {scores.mean():.3f} +/- {scores.std():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A tree with an optimized depth is better than linear regression for 10 CV iterations out of 10 folds.\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"A tree with an optimized depth is better than linear regression for \"\n",
    "    f\"{sum(cv['test_score'] > cv_results_tree_optimal_depth['test_score'])} CV \"\n",
    "    \"iterations out of 10 folds.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A tree model using both numerical and categorical features is better than a tree with optimal depth using only numerical features for 8 CV iterations out of 10 folds.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "categorical_processor = OrdinalEncoder(\n",
    "    handle_unknown=\"use_encoded_value\", unknown_value=-1\n",
    ")\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    (categorical_processor, selector(dtype_include=object)),\n",
    "    (\"passthrough\", selector(dtype_exclude=object))\n",
    ")\n",
    "tree = make_pipeline(preprocessor, DecisionTreeRegressor(max_depth=7, random_state=0))\n",
    "\n",
    "cv_results = cross_validate(\n",
    "    tree, data, target, cv=10, return_estimator=True, n_jobs=2\n",
    ")\n",
    "cv_results[\"test_score\"].mean()\n",
    "print(\n",
    "    \"A tree model using both numerical and categorical features is better than a \"\n",
    "    \"tree with optimal depth using only numerical features for \"\n",
    "    f\"{sum(cv_results['test_score'] > cv_results_tree_optimal_depth['test_score'])} CV \"\n",
    "    \"iterations out of 10 folds.\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
